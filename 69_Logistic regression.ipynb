{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3f8149",
   "metadata": {},
   "source": [
    "`Basics`    \n",
    "Binary classification problems are typically supervised learning problems, meaning that the dataset includes labeled examples. The model learns from these labeled data points to make predictions about unseen data.      \n",
    "A binary classification problem is a type of classification problem where the goal is to categorize data into one of two distinct classes or categories. Each data point is assigned a label of either 0 or 1 (or any two labels, such as \"True\"/\"False\", \"Yes\"/\"No\", \"Spam\"/\"Not Spam\", etc.).     \n",
    "\n",
    "Examples of Binary Classification Problems:   \n",
    "- 6Email Spam Detection : Classes: Spam (1) or Not Spam (0)    \n",
    "Objective: Predict whether an email is spam or not based on features such as the sender, subject, and the content of the email.    \n",
    "- Medical Diagnosis : Classes: Disease (1) or No Disease (0)   \n",
    "Objective: Predict whether a patient has a specific disease based on medical tests and symptoms.        \n",
    "- Credit Card Fraud Detection : Classes: Fraudulent (1) or Non-Fraudulent (0)    \n",
    "Objective: Predict whether a transaction is fraudulent or not based on transaction details.           \n",
    "- Customer Churn Prediction : Classes: Churn (1) or No Churn (0)      \n",
    "Objective: Predict whether a customer will leave (churn) or stay with a company based on customer data like purchase history, customer service interactions, and account activity.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41224263",
   "metadata": {},
   "source": [
    "- Linearly Separable Data: When the data can be perfectly separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions), logistic regression works well and can find the optimal linear decision boundary.\n",
    "\n",
    "- Non-Linearly Separable Data: In real-world datasets, the data is often not linearly separable. In these cases, logistic regression might not perform well unless transformations (like polynomial features) or other models (like SVMs or neural networks) are used to handle the non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1ba15",
   "metadata": {},
   "source": [
    "<a>Logistice regression :  </a>    \n",
    "Logistic regression is a statistical method used for binary classification problems, where the outcome variable is categorical with two possible classes (often labeled as 0 and 1).   \n",
    "It is commonly used in machine learning for tasks like spam detection, medical diagnoses, and predicting outcomes such as whether a customer will buy a product.    \n",
    "In Logistic Regression, the goal is to classify data points into two classes by finding a decision boundary, which is typically represented as a line (in two-dimensional data) or a hyperplane (in higher dimensions). This decision boundary separates the two classes based on the features of the data.     \n",
    "\n",
    "Logistic regression can be understood from two key perspectives: \"Geometric\" and \"Probabilistic\".    \n",
    "- Geometric Perspective: Logistic regression is about finding a linear decision boundary (hyperplane) that separates the two classes in the feature space. The model finds the optimal line (or hyperplane) that maximizes the separation between classes.   \n",
    "- Probabilistic Perspective: Logistic regression models the probability of the class being 1 using the logistic (sigmoid) function. It uses a probabilistic approach to estimate the likelihood of class membership and applies a decision threshold to make the final classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7341b38",
   "metadata": {},
   "source": [
    "- These are two ways to find the optimal weights (parameters) for the logistic regression model, but they are based on different underlying principles.    \n",
    "1. Perceptron Approach (also known as the Gradient Descent approach)\n",
    "2. Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a97fe",
   "metadata": {},
   "source": [
    "the conclusio to find the ABC and N value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d72b7",
   "metadata": {},
   "source": [
    "1. Perceptron Approach (Gradient Descent)\n",
    "The Perceptron approach to learning logistic regression is based on the idea of iteratively improving the model's parameters (weights) using a learning algorithm like Gradient Descent.\n",
    "\n",
    "Key Concepts:\n",
    "Goal: The aim is to find the optimal set of weights (Î¸) that minimizes the loss function (specifically, the logistic loss or cross-entropy loss).\n",
    "Learning Rule: The perceptron approach (often applied to linear classification and used for logistic regression) updates the weights in the direction of the negative gradient of the loss function.\n",
    "\n",
    "Process:\n",
    "Initialization: Start with random weights for the logistic regression model (Î¸0,Î¸1,â€¦,Î¸n).\n",
    "Prediction: For each input ğ‘¥ğ‘–, compute the predicted probability using the logistic function â„ğœƒ(ğ‘¥)=1/1+ğ‘’âˆ’ğœƒğ‘‡.\n",
    "â€‹\n",
    "\n",
    "\n",
    "Error Calculation: Compute the error for each prediction:\n",
    "Error=ğ‘¦ğ‘–âˆ’â„ğœƒ(ğ‘¥ğ‘–)\n",
    "Error=yiâˆ’hÎ¸(xi)\n",
    "where ğ‘¦ğ‘– is the true label (0 or 1) for the data point ğ‘¥ğ‘–.\n",
    "Gradient Descent: Update the weights in the direction that reduces the error:\n",
    "\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "â†\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "+\n",
    "ğ›¼\n",
    "â‹…\n",
    "Error\n",
    "â‹…\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "ğ‘—\n",
    "Î¸ \n",
    "j\n",
    "â€‹\n",
    " â†Î¸ \n",
    "j\n",
    "â€‹\n",
    " +Î±â‹…Errorâ‹…x \n",
    "ij\n",
    "â€‹\n",
    " \n",
    "where \n",
    "ğ›¼\n",
    "Î± is the learning rate, and \n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "ğ‘—\n",
    "x \n",
    "ij\n",
    "â€‹\n",
    "  is the feature value for the \n",
    "ğ‘–\n",
    "i-th training example and \n",
    "ğ‘—\n",
    "j-th feature.\n",
    "\n",
    "Repeat: Continue the process for all training data points in multiple iterations (epochs) until the model converges to an optimal set of weights.\n",
    "\n",
    "Advantages of Perceptron/Gradient Descent:\n",
    "Simple and Intuitive: The idea behind the perceptron and gradient descent is easy to understand, making it a good starting point for learning machine learning algorithms.\n",
    "Flexibility: Gradient descent can be used in various optimization problems, including those for logistic regression.\n",
    "Efficiency: Once the algorithm converges, it gives us the optimal weights that minimize the logistic loss function.\n",
    "\n",
    "Challenges:\n",
    "Learning Rate: Selecting an appropriate learning rate (Î±) is crucial. If itâ€™s too large, the algorithm may fail to converge; if itâ€™s too small, the algorithm may take too long to converge.\n",
    "Local Minima: Logistic regression typically converges to a global minimum for convex loss functions (like the logistic loss), but care must still be taken when using other algorithms that involve non-convex optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b8bec",
   "metadata": {},
   "source": [
    "2. Maximum Likelihood Estimation (MLE) Approach\n",
    "The Maximum Likelihood Estimation (MLE) approach to learning logistic regression focuses on finding the set of weights that maximize the likelihood of the observed data under the model.\n",
    "\n",
    "Key Concepts:\n",
    "Likelihood Function: The likelihood of a set of parameters \n",
    "ğœƒ\n",
    "Î¸ is the probability of observing the given set of data points.\n",
    "\n",
    "For binary classification, the likelihood for a single example is given by the logistic function:\n",
    "\n",
    "ğ‘ƒ\n",
    "(\n",
    "ğ‘¦\n",
    "=\n",
    "1\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ";\n",
    "ğœƒ\n",
    ")\n",
    "=\n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ğ‘’\n",
    "âˆ’\n",
    "ğœƒ\n",
    "ğ‘‡\n",
    "ğ‘¥\n",
    "P(y=1âˆ£x;Î¸)=h \n",
    "Î¸\n",
    "â€‹\n",
    " (x)= \n",
    "1+e \n",
    "âˆ’Î¸ \n",
    "T\n",
    " x\n",
    " \n",
    "1\n",
    "â€‹\n",
    " \n",
    "and for \n",
    "ğ‘¦\n",
    "=\n",
    "0\n",
    "y=0, it is:\n",
    "\n",
    "ğ‘ƒ\n",
    "(\n",
    "ğ‘¦\n",
    "=\n",
    "0\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ";\n",
    "ğœƒ\n",
    ")\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    ")\n",
    "P(y=0âˆ£x;Î¸)=1âˆ’h \n",
    "Î¸\n",
    "â€‹\n",
    " (x)\n",
    "Log-Likelihood Function: The log-likelihood function is the sum of the logarithms of the individual likelihoods for all data points in the dataset. Maximizing this function is equivalent to finding the optimal parameters \n",
    "ğœƒ\n",
    "Î¸ that best fit the data.\n",
    "\n",
    "The log-likelihood function for logistic regression is:\n",
    "\n",
    "ğ¿\n",
    "(\n",
    "ğœƒ\n",
    ")\n",
    "=\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘š\n",
    "[\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "log\n",
    "â¡\n",
    "(\n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    ")\n",
    "log\n",
    "â¡\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    ")\n",
    "]\n",
    "L(Î¸)= \n",
    "i=1\n",
    "âˆ‘\n",
    "m\n",
    "â€‹\n",
    " [y \n",
    "i\n",
    "â€‹\n",
    " log(h \n",
    "Î¸\n",
    "â€‹\n",
    " (x \n",
    "i\n",
    "â€‹\n",
    " ))+(1âˆ’y \n",
    "i\n",
    "â€‹\n",
    " )log(1âˆ’h \n",
    "Î¸\n",
    "â€‹\n",
    " (x \n",
    "i\n",
    "â€‹\n",
    " ))]\n",
    "where \n",
    "ğ‘š\n",
    "m is the number of training examples, and \n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "y \n",
    "i\n",
    "â€‹\n",
    "  is the actual label (0 or 1).\n",
    "\n",
    "Objective: The objective is to maximize the log-likelihood function to find the best-fitting parameters \n",
    "ğœƒ\n",
    "Î¸. In practice, we typically minimize the negative log-likelihood, which leads us to the logistic loss (or cross-entropy loss) used in logistic regression.\n",
    "\n",
    "Process:\n",
    "Initialize Weights: Start with initial weights \n",
    "ğœƒ\n",
    "0\n",
    ",\n",
    "ğœƒ\n",
    "1\n",
    ",\n",
    "â€¦\n",
    ",\n",
    "ğœƒ\n",
    "ğ‘›\n",
    "Î¸ \n",
    "0\n",
    "â€‹\n",
    " ,Î¸ \n",
    "1\n",
    "â€‹\n",
    " ,â€¦,Î¸ \n",
    "n\n",
    "â€‹\n",
    " .\n",
    "\n",
    "Compute Predictions: For each data point \n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "x \n",
    "i\n",
    "â€‹\n",
    " , compute the predicted probability \n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ğ‘’\n",
    "âˆ’\n",
    "ğœƒ\n",
    "ğ‘‡\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "h \n",
    "Î¸\n",
    "â€‹\n",
    " (x \n",
    "i\n",
    "â€‹\n",
    " )= \n",
    "1+e \n",
    "âˆ’Î¸ \n",
    "T\n",
    " x \n",
    "i\n",
    "â€‹\n",
    " \n",
    " \n",
    "1\n",
    "â€‹\n",
    " .\n",
    "\n",
    "Maximize Log-Likelihood: Use an optimization algorithm like gradient descent to find the values of \n",
    "ğœƒ\n",
    "Î¸ that maximize the log-likelihood function.\n",
    "\n",
    "Optimization: The gradient of the log-likelihood with respect to the parameters \n",
    "ğœƒ\n",
    "ğ‘—\n",
    "Î¸ \n",
    "j\n",
    "â€‹\n",
    "  is:\n",
    "\n",
    "âˆ‚\n",
    "ğ¿\n",
    "âˆ‚\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "=\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘š\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "â„\n",
    "ğœƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    ")\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "ğ‘—\n",
    "âˆ‚Î¸ \n",
    "j\n",
    "â€‹\n",
    " \n",
    "âˆ‚L\n",
    "â€‹\n",
    " = \n",
    "i=1\n",
    "âˆ‘\n",
    "m\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’h \n",
    "Î¸\n",
    "â€‹\n",
    " (x \n",
    "i\n",
    "â€‹\n",
    " ))x \n",
    "ij\n",
    "â€‹\n",
    " \n",
    "Use this gradient to iteratively update the parameters:\n",
    "\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "â†\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "+\n",
    "ğ›¼\n",
    "â‹…\n",
    "âˆ‚\n",
    "ğ¿\n",
    "âˆ‚\n",
    "ğœƒ\n",
    "ğ‘—\n",
    "Î¸ \n",
    "j\n",
    "â€‹\n",
    " â†Î¸ \n",
    "j\n",
    "â€‹\n",
    " +Î±â‹… \n",
    "âˆ‚Î¸ \n",
    "j\n",
    "â€‹\n",
    " \n",
    "âˆ‚L\n",
    "â€‹\n",
    " \n",
    "Convergence: Repeat the process until the algorithm converges to the optimal set of parameters.\n",
    "\n",
    "Advantages of MLE:\n",
    "Theoretical Foundation: MLE is grounded in probability theory and gives us a principled way of fitting the model based on the likelihood of the data.\n",
    "\n",
    "Interpretability: The interpretation of the coefficients in terms of probabilities is clear and natural, as they correspond to odds ratios.\n",
    "\n",
    "Global Solution: Logistic regressionâ€™s log-likelihood function is convex, meaning that gradient descent will converge to the global maximum.\n",
    "\n",
    "Challenges:\n",
    "Optimization Complexity: In practice, MLE often requires using optimization techniques like gradient descent to maximize the log-likelihood, especially when the number of features or data points is large.\n",
    "\n",
    "Computational Cost: If the dataset is large or high-dimensional, the optimization process can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf973e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55cfc7d1",
   "metadata": {},
   "source": [
    "In Summary:   \n",
    "The Perceptron approach (often using gradient descent) focuses on minimizing the logistic loss function by iteratively adjusting the weights.   \n",
    "The Maximum Likelihood Estimation (MLE) approach focuses on maximizing the likelihood of the data under the model, providing a probabilistic foundation for logistic regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33489776",
   "metadata": {},
   "source": [
    "loop(1000(epocs)) times\n",
    "till y convergence, how may point are missqualified, when missquilified is 0 then convergence is done and loop stop...  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
